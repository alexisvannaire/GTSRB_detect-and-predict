{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b564c949",
   "metadata": {},
   "source": [
    "# Traffic signs detection and classification with Detecto and Tensorflow \n",
    "\n",
    "### Part 3 - *Classification*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21eb52e",
   "metadata": {},
   "source": [
    "All the functions and visualizations I used here can be found on my GitHub page: [https://github.com/alexisvannaire/GTSRB_detect-and-predict](https://github.com/alexisvannaire/GTSRB_detect-and-predict)\n",
    "\n",
    "See the first part (1_detect-and-predict) if you need details on the packages I used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a6a231",
   "metadata": {},
   "source": [
    "**For this notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff401d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import json\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "## python files\n",
    "import plots # plots.py\n",
    "import process_data # process_data.py\n",
    "import calculations # calculations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1063e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False for default visualizations and computations\n",
    "gtsrb_exists = False # True if you've placed the gtsrb dataset in the \"./data/gtsrb/\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b7664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "n_classes = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837792bb",
   "metadata": {},
   "source": [
    "# IV. Classification Model\n",
    "\n",
    "## 1. Models and preprocessing\n",
    "\n",
    "For the classification task, we're going to train two types of models: standard CNNs and MobileNets.\n",
    "\n",
    "### i) standard CNNs\n",
    "\n",
    "A CNN is a type of neural networks using convolutional layers in order to reduce the dimensionality of data and extract features.\n",
    "They're known to be used in image classification tasks.\n",
    "\n",
    "In fact, there are succesive blocks composed of one convolutional layer and one pooling layer. The convolution creates features maps and the pooling reduces the dimentionality.\n",
    "\n",
    "So the idea is that we start with large images (\"high\" dimensions in terms of height and width), then as we progress through the layers we reduce the image size and increase the depth. At the end, when we flatten the last feature map, we get a feature vector which will be used to classify images through a classical neural network (fully connected layer). \n",
    "\n",
    "Here's how you can create CNN models with tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_simple_CNN_model(batch_size, img_height, img_width, n_classes):\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        # First conv layer: 64 filters with size 3x3\n",
    "        tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                              input_shape=(batch_size, img_height, img_width, 3)), # input shape\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        # Second layer: 128 filters with size 3x3\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        # Third layer: 256 filters with size 3x3 \n",
    "        tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        C,\n",
    "        # Flatten layer: convert the features map into a vector\n",
    "        tf.keras.layers.Flatten(),\n",
    "        # The fully connected layer (128->64->n_classes) try to classify features into the classes we want\n",
    "        tf.keras.layers.Dense(units=128, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dense(units=64, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        # The ooutput layer: has to be as unit as the class number we have\n",
    "        tf.keras.layers.Dense(units=n_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebd214",
   "metadata": {},
   "source": [
    "I trained 3 CNNs with the same architecture but with several input shapes.\n",
    "\n",
    "As we've seen, the majority of images sizes are squares between 30 and 100 pixels.\n",
    "So I've trained the first one on 30x30 pixels resized images, then another one on 60x60 pixels resized images and the last on 90x90 pixels resized images (for 50 epochs each).\n",
    "\n",
    "After, I added Data Augmentation and started with these models as a base and trained them for more epochs. \n",
    "\n",
    "Here's how I named them:\n",
    "\n",
    "* CNN_30-30_50e\n",
    "* CNN_60-60_50e\n",
    "* CNN_90-90_50e\n",
    "* CNN_30-30_50e_DA-50e\n",
    "* CNN_60-60_50e_DA_50e\n",
    "* CNN_90-90_50e_DA-30e\n",
    "\n",
    "The architecture I used is this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860ea3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_CNN_model(batch_size, img_height, img_width, n_classes):\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        # Rescaling images: to get pixel values between 0 and 1 instead of 0 and 255 \n",
    "        tf.keras.layers.Rescaling(1./255),\n",
    "        # First conv layer: 64 filters with size 7x7\n",
    "        tf.keras.layers.Conv2D(filters=64, kernel_size=7, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                              input_shape=(batch_size, img_height, img_width, 3)), # input shape\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        # Second layer: 2 conv. layers with 128 filters with size 3x3 \n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        # Third layer: 2 conv. with 256 filters with size 3x3 \n",
    "        tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        # Flatten layer: convert the features map into a vector\n",
    "        tf.keras.layers.Flatten(),\n",
    "        # The fully connected layer (128->64->n_classes) try to classify features into the classes we want\n",
    "        tf.keras.layers.Dense(units=128, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=64, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # The output layer: has to be as unit as the class number we have\n",
    "        tf.keras.layers.Dense(units=n_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b52200",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height, img_width = 30, 30\n",
    "cnn1 = init_CNN_model(batch_size, img_height, img_width, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2202c4",
   "metadata": {},
   "source": [
    "The changes you can see from the basic models I showed earlier are as follows:\n",
    "\n",
    "* There's a rescaling layer that normalizes values to the interval $[0, 1]$ (neural networks work better with values into the intervals $[0, 1]$ or $[-1, 1]$). But of course you could do it directly on your data instead.\n",
    "* There are two convolutional layers before the pooling one in layers 2 and 3.\n",
    "* I added Dropout layers in the fully connected layer. \n",
    "This layer deactivates a random percentage of neurons at each step during training time.\n",
    "This prevents the model from falling into overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412767b7",
   "metadata": {},
   "source": [
    "### ii) MobileNets\n",
    "\n",
    "MobileNet models have been developed with the aim of enabling the use of computer vision models for mobile and embedded applications.\n",
    "\n",
    "Indeed, you can develop a great model really accurate but with so much parameters that the prediction speed is pretty low and the required memory size is too large to be handled by phones.\n",
    "\n",
    "*\"MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks.\"*\n",
    "\n",
    "For more details you can see this article: https://arxiv.org/abs/1704.04861\n",
    "    \n",
    "Here you can see its architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf1c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(\"imgs/mobilenet_architecture.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c285a",
   "metadata": {},
   "source": [
    "You can easily get this model with Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91121ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_net = tf.keras.applications.mobilenet.MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89a63a3",
   "metadata": {},
   "source": [
    "You can then choose to make the pre-trained weights trainable or not.\n",
    "\n",
    "But you should most of the time train the fully connected layer you added at first before thinking of the pre-trained layers. \n",
    "This is an optimized pre-trained model that can extract relevant features from your images. \n",
    "So at first you should just try to train the fully connected layer, it will aim to see if by combining these features it can predict the right classes or not.\n",
    "\n",
    "To do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in mobile_net.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b134b",
   "metadata": {},
   "source": [
    "Then, you can add your fully connected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eb6263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mobilenet_model(mobile_net, n_classes):\n",
    "\n",
    "    mobile_net_1 = keras.Sequential([\n",
    "        mobile_net, # The MobileNet model (without the top, because of the 'include_top=False' we provided in its definition)\n",
    "        # A pooling layer that will return a flatten vector\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        # Fully connected layer with dropouts\n",
    "        keras.layers.Dense(1024, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(512, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    return mobile_net_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea20856b",
   "metadata": {},
   "source": [
    "I've trained 3 models:\n",
    "\n",
    "The first one, with only the fully connected layer trainable for 100 epochs.\n",
    "The second one is based on the first but letting some of the MobileNet layers trainable for another 20 epochs.\n",
    "And the last one is based on the second but adding Data Augmentation for 50 epochs.\n",
    "\n",
    "Here are their names:\n",
    "\n",
    "* MobileNet_224-224_100e\n",
    "* MobileNet_224-224_120e\n",
    "* MobileNet_224-224_120e_DA-50e\n",
    "\n",
    "If you want to let some pre-trained layers being trainable you juste have to specify it before you initialize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d0244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_net = tf.keras.applications.mobilenet.MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "trainable_index = 61\n",
    "for layer in mobile_net.layers[:trainable_index]:\n",
    "    layer.trainable=True\n",
    "for layer in mobile_net.layers[trainable_index:]:\n",
    "    layer.trainable=False\n",
    "    \n",
    "model = init_mobilenet_model(mobile_net, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf404e1",
   "metadata": {},
   "source": [
    "### iii) Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a9513d",
   "metadata": {},
   "source": [
    "Now we're going to see how to create training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b69174",
   "metadata": {},
   "source": [
    "As the GTSRB dataset is already structured for learning we're going to use `tf.keras.utils.image_dataset_from_directory`.\n",
    "\n",
    "This function creates a set just by using the folder structure.\n",
    "\n",
    "The `Train` folder contains a list of folders corresponding to each class where images are stored:\n",
    "\n",
    "```\n",
    "gtsrb\n",
    "│ \n",
    "├── Train\n",
    "│   ├── 0\n",
    "│   │   ├── 00000_00000_00000.png\n",
    "│   │   ├── 00000_00000_00001.png\n",
    "│   │   ├── ...\n",
    "│   │   └── 00000_00006_00029.png\n",
    "│   ├── 1\n",
    "│   ├── 2\n",
    "│   ├── ...\n",
    "│   └── 42\n",
    "├── Test\n",
    "│   ├── 00000.png\n",
    "│   ├── 00001.png\n",
    "│   ├── ...\n",
    "│   └── 12629.png\n",
    "├── Meta\n",
    "│   ├── 0.png\n",
    "│   ├── 1.png\n",
    "│   ├── ...\n",
    "│   └── 42.png\n",
    "├── GT-final_test.csv\n",
    "├── Meta.csv\n",
    "├── Test.csv\n",
    "└── Train.csv\n",
    "```\n",
    "\n",
    "(I moved the `GT-final_test.csv` file from the Test folder in order to handle easily the test part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c37cf6f",
   "metadata": {},
   "source": [
    "It's important to keep class names in the order you want to not be confused with predictions classes later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38854282",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gtsrb_exists:\n",
    "    data_dir = \"data/gtsrb/dataset/\"\n",
    "    train_data_dir = data_dir+\"Train/\"\n",
    "    test_data_dir = data_dir+\"Test/\"\n",
    "\n",
    "    class_names = os.listdir(train_data_dir)\n",
    "    n_classes = len(class_names)\n",
    "    class_names_int = sorted(list(map(lambda x: int(x), class_names)))\n",
    "else:\n",
    "    class_names = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9966b4b2",
   "metadata": {},
   "source": [
    "Then, you just have to give the folderpath of your dataset and choose some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gtsrb_exists:\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"data/gtsrb/Train/\",\n",
    "        label_mode='int', # labels are encoded as integers\n",
    "        class_names=class_names, # list of class names (they've to match the names of subdirectories)\n",
    "        batch_size=batch_size, # batch_size\n",
    "        image_size=(img_height, img_width), # size to resize images to after they are read from disk\n",
    "        seed=seed, # seed, to ensure reproductibility (~ random_state)\n",
    "        validation_split=0.2, # percentage of data you want in your validation set\n",
    "        subset=\"training\", # for training set creation\n",
    "        crop_to_aspect_ratio=False # if True, resize the images without aspect ratio distortion\n",
    "    )\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"data/gtsrb/Train/\",\n",
    "        label_mode='int', # labels are encoded as integers\n",
    "        class_names=class_names, # list of class names (they've to match the names of subdirectories)\n",
    "        batch_size=batch_size, # batch_size\n",
    "        image_size=(img_height, img_width), # size to resize images to after they are read from disk\n",
    "        seed=seed, # seed, to ensure reproductibility (~ random_state)\n",
    "        validation_split=0.2, # percentage of data you want in your validation set\n",
    "        subset=\"validation\", # for validation set creation\n",
    "        crop_to_aspect_ratio=False # if True, resize the images without aspect ratio distortion\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad38089",
   "metadata": {},
   "source": [
    "The split will be valid only if you give the same seed to both calls.\n",
    "Otherwise, you may find the same data in both sets and not find some either.\n",
    "\n",
    "And if you prefer, you can also create them this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f01be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gtsrb_exists:\n",
    "    train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"data/gtsrb/Train/\",\n",
    "        label_mode='int', # labels are encoded as integers\n",
    "        class_names=class_names, # list of class names (they've to match the names of subdirectories)\n",
    "        batch_size=batch_size, # batch_size\n",
    "        image_size=(img_height, img_width), # size to resize images to after they are read from disk\n",
    "        seed=seed, # seed, to ensure reproductibility (~ random_state)\n",
    "        validation_split=0.2, # percentage of data you want in your validation set\n",
    "        subset=\"both\", # for training and validation sets creation\n",
    "        crop_to_aspect_ratio=False # if True, resize the images without aspect ratio distortion\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5987b52",
   "metadata": {},
   "source": [
    "(For more details you can take a look here: https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a9e151",
   "metadata": {},
   "source": [
    "That's it! Let's see how to train a model now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4050453a",
   "metadata": {},
   "source": [
    "## 2. Training\n",
    "\n",
    "Training the models is as simple as calling the `compile` and `fit` methods.\n",
    "\n",
    "But there are a lot of parameters you can tune that can have a strong effect on the quality of the training.\n",
    "\n",
    "### i) compilation\n",
    "\n",
    "When you compile a model you configure it for training.\n",
    "\n",
    "You have to choose an optimizer, which is an algorithm that optimizes model weights to minimize the loss function during the training.\n",
    "\n",
    "Here are the most used ones:\n",
    "\n",
    "* SGD (Stochastic Gradient Descent)\n",
    "* AdaGrad (Adaptive Gradient Descent)\n",
    "* RMS-Prop (Root Mean Square Propagation)\n",
    "* AdaDelta\n",
    "* Adam (Adaptive Moment Estimation)\n",
    "* Nadam (Adam with Nesterov momentum)\n",
    "* Ftrl (Follow The Regularized Leader)\n",
    "\n",
    "Here's the corresponding string list: `'sgd'`, `'adagrad'`, `'rmsprop'`, `'adadelta'`, `'adam'`, `'nadam'`, `'ftrl'`.\n",
    "\n",
    "<br>\n",
    "\n",
    "Also, you have to choose the loss, which is the way you want the errors to be computed and will be optimized (often minimized) by the optimizer.\n",
    "\n",
    "This choice depends on your task, for example if it's a:\n",
    "\n",
    "* **Regression task:** \n",
    "    + Mean Squared Error\n",
    "    + Mean Absolute Error\n",
    "    + Log-Cosh Loss\n",
    "* **Binary classification:**\n",
    "    + Binary Cross-Entropy\n",
    "    + Hinge Loss\n",
    "    + Squared Hinge Loss\n",
    "* **Multi-class Classification (which is our case):**\n",
    "    + Categorical Cross-Entropy\n",
    "    + Sparse Categorical Cross-Entropy\n",
    "    + Kullback-Leibler Divergence\n",
    "    \n",
    "Here's the corresponding string list: `'mean_squared_error'`, `'mean_absolute_error'`, `'logcosh'`, `'binary_crossentropy'`, `'hinge'`, `'squared_hinge'`, `'categorical_crossentropy'`, `'sparse_categorical_crossentropy'`, `'kullback_leibler_divergence'`.\n",
    "\n",
    "<br>\n",
    "\n",
    "Then, you have to choose the metric. Which is the way you want the performance of the model to be computed.\n",
    "\n",
    "The most known and used are: \n",
    "\n",
    "* **Classification:**\n",
    "    + `'accuracy'`\n",
    "    + `'precision'`\n",
    "    + `'recall'`\n",
    "    + `'f1_score'`\n",
    "* **Regression:**\n",
    "    + `'mean_squared_error'`\n",
    "    + `'mean_absolute_error'`.\n",
    "\n",
    "We will use this configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", \n",
    "    optimizer=\"nadam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a51bc9",
   "metadata": {},
   "source": [
    "### ii) fit\n",
    "\n",
    "When you call the fit function you basically train your model.\n",
    "\n",
    "There are interesting parameters you can tune:\n",
    "\n",
    "* **epochs**: times number the model will pass through all data\n",
    "* **callbacks**: methods that allow us to stop and/ or save the model according to some conditions\n",
    "* **use_multiprocessing**: if you want to accelerate the training using multiprocessing\n",
    "* **batch_size** (we won't specify it because we've already done it in the training and validation datasets)\n",
    "\n",
    "And a lot of others, you can check them here: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
    "\n",
    "Here's the callback method we're going to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163f60a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"models/classifier/\"\n",
    "model_name = \"MobileNet_224-224\"\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "            models_path+model_name+'/checkpoint',\n",
    "            monitor=\"val_accuracy\",\n",
    "            save_weights_only=True,\n",
    "            save_best_only=True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f03a2b",
   "metadata": {},
   "source": [
    "It allows us to save the model weights each time it performs better.\n",
    "\n",
    "And now we can train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f57a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "if gtsrb_exists:\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        use_multiprocessing=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f1a38",
   "metadata": {},
   "source": [
    "You can save the weights of the model by using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54395c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f\"models/classifier/{model_name}/model_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f692da",
   "metadata": {},
   "source": [
    "In order to load and use the model later you'll have to:\n",
    "    \n",
    "* initialize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa9df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_mobilenet_model(mobile_net, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf49fc2c",
   "metadata": {},
   "source": [
    "* load weights: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c444bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(f\"models/classifier/{model_name}/model_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a26f153",
   "metadata": {},
   "source": [
    "* compile the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", \n",
    "    optimizer=\"nadam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc6c46a",
   "metadata": {},
   "source": [
    "Don't forget the compilation, otherwise it won't work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00747a7",
   "metadata": {},
   "source": [
    "Let's come back to the training part:\n",
    "\n",
    "The `fit` function returns a History object containing training loss and accuracy values.\n",
    "\n",
    "By the way, you can save it this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a130cd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gtsrb_exists:\n",
    "    json.dump(history.history, open(f\"models/classifier/{model_name}/history.json\", 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a450e2",
   "metadata": {},
   "source": [
    "Let's see it for the trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe43429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_training_folderpath = \"imgs/models_training/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d45a77",
   "metadata": {},
   "source": [
    "*CNN 30x30: 50 epochs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CNN1_30-30_50e\"\n",
    "plots.plot_model_training(\"image\", model_name,\n",
    "    image_folderpath=models_training_folderpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d5305f",
   "metadata": {},
   "source": [
    "For each model, there are the loss evolution on the left and the accuracy evolution on the right.\n",
    "\n",
    "You can see here that the loss decreases quickly close to zero. \n",
    "The same thing happens for accuracy: the values increase close to 1 within several epochs (which is what we want).\n",
    "\n",
    "Something important you can notice it is that validation loss and validation accuracy is mostly better than the train ones.\n",
    "Usually you would expect the opposite happening. \n",
    "The reason is that we've used dropout layers in these models, and they're active only during the training step.\n",
    "So it's obviously harder for the model to be as accurate during the training step than in the validation one.\n",
    "And you can see it easily by evaluating you model on you training set after the training: it'll have way better scores.\n",
    "\n",
    "The same things will happen for the other standard CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a1d19",
   "metadata": {},
   "source": [
    "*CNN 60x60: 50 epochs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CNN1_60-60_50e\"\n",
    "plots.plot_model_training(\"image\", model_name, \n",
    "    image_folderpath=models_training_folderpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde8382e",
   "metadata": {},
   "source": [
    "*CNN 90x90: 50 epochs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eecf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CNN1_90-90_50e\"\n",
    "plots.plot_model_training(\"image\", model_name, \n",
    "    image_folderpath=models_training_folderpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1212687a",
   "metadata": {},
   "source": [
    "*MobileNet 224x224: 120 epochs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882acd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MobileNet_1_224-224_120e\"\n",
    "plots.plot_model_training(\"image\", model_name, \n",
    "    image_folderpath=models_training_folderpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc104fd8",
   "metadata": {},
   "source": [
    "Here there are two models we can display in a single view: MobileNet 224x224: 100 epochs and MobileNet 224x224: 120 epochs.\n",
    "\n",
    "The first one has been trained on the top layers (the non pre-trained part), the second one is based on the first and has been trained for 20 additional epochs on both top layers and some pre-trained layers.\n",
    "\n",
    "We can see that MobileNet performs better when you allow it to adapt its pre-trained weights. \n",
    "But it takes more time to train because there are a lot of neurons/weights.\n",
    "A good way to approach the training of a pre-trained model is to train it on the top layers and make some of the pre-trained layers trainable when the model already performs rather well or begins to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f973f05",
   "metadata": {},
   "source": [
    "**Data Augmentation**\n",
    "\n",
    "Data Augmentation allows you to add variability in your datasets.\n",
    "You change your data with some filters or methods so that the model can see a larger range of possible data.\n",
    "In fact this technique can really help to avoid overfitting.\n",
    "\n",
    "You can see it as adding noise in your data so that your model will be forced to be more focus on \"relevant\" features in order to get the best predictions.\n",
    "\n",
    "It's also really helpful when you have few data and/or few data in some classes.\n",
    "\n",
    "In the following graphs you'll see that when you add Data Augmentation on your pre-trained model (the model you trained without DA), the loss and the accuracy get worse.\n",
    "It's as if the model is learning all over again.\n",
    "But if you've tuned well your DA, you could give to your model a better generalization ability.\n",
    "That is, it'll be better on data it had never seen.\n",
    "\n",
    "Here are the methods used to add DA in these models:\n",
    "    \n",
    "* **Rescaling** with the `1./255` value will just rescale data in the [0, 1] range instead of the [0, 255] one. This isn't a Data Augmentation layer.\n",
    "* **RandomContrast** will randomly adjust the contrast of images according to a contrast factor. We set it at 0.1 which means a random constrast between -0.1 and 0.1.\n",
    "* **RandomRotation** will randomy rotate images according to the factor. I've chosen 0.2 so that rotations will be randomly between around -72 and 72 degrees ($\\pm 0.2 \\times 2 \\pi$). Which is surely a too large range (for traffic signs that can have symmetrical symbols).\n",
    "* **RandomZoom** will zoom randomly in or out images. Here the factor is 0.1 which will zoom in within the [-0.1, 0.1] range (or [-10%, +10%]).\n",
    "* **RandomTranslation** will randomly translate images heightwise and widthwise. Here, with 0.1 for height and 0.1 for width, we will have images translated heightwise and widthwise within the [-10%, +10%] range.\n",
    "* **Resizing** isn't a data augmentation layer here, because we just want to be sure the outputs images keep the right dimensions.\n",
    "\n",
    "The more you will add variability the mode difficult it will be for your model to learn.\n",
    "So you have to test it and try to find the right balance.\n",
    "Not enough variability won't have any effect, and too much could confuse or get the model lost.\n",
    "\n",
    "And here's how to add it in your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5a12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.RandomContrast(0.1),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    tf.keras.layers.RandomTranslation(0.1, 0.1),\n",
    "    tf.keras.layers.Resizing(img_height, img_width),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978cd00f",
   "metadata": {},
   "source": [
    "Then, you just have to add it at the begining of your model initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865f1cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_CNN_model_with_da(batch_size, img_height, img_width, n_classes):\n",
    "    \n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.Rescaling(1./255),\n",
    "        tf.keras.layers.RandomContrast(0.1),\n",
    "        tf.keras.layers.RandomRotation(0.2),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "        tf.keras.layers.RandomTranslation(0.1, 0.1),\n",
    "        tf.keras.layers.Resizing(img_height, img_width),\n",
    "    ])\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        # Data Augmentation\n",
    "        data_augmentation,\n",
    "        # First conv layer: 64 filters with size 7x7\n",
    "        tf.keras.layers.Conv2D(filters=64, kernel_size=7, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                              input_shape=(batch_size, img_height, img_width, 3)), # input shape\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        # Second layer: 2 conv. layers with 128 filters with size 3x3 \n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        # Third layer: 2 conv. with 256 filters with size 3x3 \n",
    "        tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        # Flatten layer: convert the features map into a vector\n",
    "        tf.keras.layers.Flatten(),\n",
    "        # The fully connected layer (128->64->n_classes) try to classify features into the classes we want\n",
    "        tf.keras.layers.Dense(units=128, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=64, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # The output layer: has to be as unit as the class number we have\n",
    "        tf.keras.layers.Dense(units=n_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5a5dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mobilenet_model_with_da(mobile_net, img_height, img_width, n_classes):\n",
    "\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.Rescaling(1./255),\n",
    "        tf.keras.layers.RandomContrast(0.1),\n",
    "        tf.keras.layers.RandomRotation(0.2),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "        tf.keras.layers.RandomTranslation(0.1, 0.1),\n",
    "        tf.keras.layers.Resizing(img_height, img_width),\n",
    "    ])\n",
    "    \n",
    "    mobile_net_1 = keras.Sequential([\n",
    "        # Data Augmentation\n",
    "        data_augmentation,\n",
    "        # The MobileNet model (without the top, because of the 'include_top=False' we provided in its definition)\n",
    "        mobile_net, \n",
    "        # A pooling layer that will return a flatten vector\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        # Fully connected layer with dropouts\n",
    "        keras.layers.Dense(1024, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(512, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02204bd0",
   "metadata": {},
   "source": [
    "If you've already trained your model and you want to continue the training, you just have to initialize your model as above and then load weights.\n",
    "\n",
    "Let's see what's happening on the trainings:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e1d4c",
   "metadata": {},
   "source": [
    "*CNN 30x30: 50 epochs + DA 50 epochs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CNN1_30-30_50e_DA-50e\"\n",
    "plots.plot_model_training(\"image\", model_name, \n",
    "    image_folderpath=models_training_folderpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3a29a",
   "metadata": {},
   "source": [
    "As you can see, from the moment you start the Data Augmentation, the model performs worse and then try to learn again how to classify the images.\n",
    "It seems the model is worse, but it could have a better generalization ability and we could be sure of that only at the test evaluation step.\n",
    "Maybe here you could have tried with more epochs.\n",
    "\n",
    "You'll see the same phenomenon again with the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d15f0e",
   "metadata": {},
   "source": [
    "*CNN 60x60: 50 epochs + DA 50 epochs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418854a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CNN1_60-60_50e_DA-50e\"\n",
    "plots.plot_model_training(\"image\", model_name, \n",
    "    image_folderpath=models_training_folderpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714cf017",
   "metadata": {},
   "source": [
    "*CNN 90x90: 50 epochs + DA 30 epochs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3be9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CNN1_90-90_50e_DA-30e\"\n",
    "plots.plot_model_training(\"image\", model_name, \n",
    "    image_folderpath=models_training_folderpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c5c34",
   "metadata": {},
   "source": [
    "*MobileNets 224x224: 120 epochs + DA 50 epochs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f631e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MobileNet_1_224-224_120e_DA-50e\"\n",
    "plots.plot_model_training(\"image\", model_name, \n",
    "    image_folderpath=models_training_folderpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f91ebc",
   "metadata": {},
   "source": [
    "Let's summarize the results on training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ab551",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(\"imgs/global_accuracy_scores_train-val.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748dd5f8",
   "metadata": {},
   "source": [
    "First of all, we have great models! They have almost all an accuracy above $98\\%$.\n",
    "\n",
    "For the CNNs, we can see that adding Data Augmentation doesn't improve the accuracies on the validation set.\n",
    "When we look at the training plots, it seems that we should have continued the training until the model stopped to learn. \n",
    "Increasing the dimension of images doesn't seem to help to improve the models. \n",
    "\n",
    "For MobileNets, the Data Augmentation has improved a bit the model. But the higher improvement comes to the moment we enabled some layers to be trainable.\n",
    "\n",
    "* The best CNN model is the `CNN_30-30_50e` with $99.9\\%$ and $99.4\\%$ accuracy on training and validation sets respectively.\n",
    "* For the MobileNets, the best is the `MobileNet_224-224_120e_DA-50e` with $99.9\\%$ and $99.8\\%$ accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bcc0b3",
   "metadata": {},
   "source": [
    "## 3. Predictions\n",
    "\n",
    "Now, let's see at the predictions of our models.\n",
    "\n",
    "Even if the predictions are rather good here, we can anlayze where the model does mistakes.\n",
    "\n",
    "To do so, we're going to use the **confusion matrix**.\n",
    "The confusion matrix is a matrix showing predicted labels versus actual labels.\n",
    "Each row represents an actual class and each column represents the predicted class.\n",
    "\n",
    "A matrix element $m_{i, j}$ is the number of times data of class $i$ has been predicted as a class $j$.\n",
    "\n",
    "Then you can see what types of errors your model does and maybe try to improve it according to them.\n",
    "\n",
    "But before, let's see how to get predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6288f69",
   "metadata": {},
   "source": [
    "* You can get them by giving it images you loaded of course, but if they're a lot it might be too much memory for your computer.\n",
    "\n",
    "   Let's say you've loaded a set of images with PIL or cv2:\n",
    "   * resize them for they match your model input size \n",
    "   * convert them into numpy arrays\n",
    "   * and create a numpy array (X) containing them all, in the way its shape will be: (n_images, img_height, img_width, n_channels)\n",
    "       \n",
    "   Finally you'll just have to run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa2ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea0adc8",
   "metadata": {},
   "source": [
    "* But here, because of the large number of images, I'll use the dataset I've created for training with the `tf.keras.utils.image_dataset_from_directory` function.\n",
    "\n",
    "    Then we have to be carefull and load these datasets each time we would get predictions.\n",
    "    \n",
    "    Here is the way I choose in order to get predictions (that's what you'll find in the `get_y_predictions` function later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106eec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gtsrb_exists:\n",
    "    y, y_probas, y_pred = [], [], []\n",
    "    ## loop on batches\n",
    "    for features, labels in train_ds:\n",
    "        # batch of labels\n",
    "        batch_labels = labels.numpy().tolist()\n",
    "        y.extend(batch_labels) # add to y list, which is the actual labels\n",
    "        # batch of images\n",
    "        X = features.numpy()\n",
    "        # predict \n",
    "        y_proba = model.predict(X)\n",
    "        y_probas.append(y_proba) # add predictions that are probability for each class\n",
    "        # best probabilities\n",
    "        best_pred_indexes = y_proba.argmax(axis=1) \n",
    "        # classes predictions\n",
    "        best_pred_classes = np.array(class_names)[best_pred_indexes]\n",
    "        y_pred.append(best_pred_classes) # add classes predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967d314",
   "metadata": {},
   "source": [
    "Check this function for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1468ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gtsrb_exists:\n",
    "    y, y_proba, y_pred = calculations.get_y_predictions(model, train_ds, class_names)\n",
    "else:\n",
    "    y, y_proba, y_pred = None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb2b20b",
   "metadata": {},
   "source": [
    "Don't forget to load dataset this way each time you want to do other predictions on it (for example with another model, or to get specific predictions, do visualizations, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b846128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#    \"data/gtsrb/Train/\",\n",
    "#    label_mode='int',\n",
    "#    class_names=class_names,\n",
    "#    batch_size=batch_size,\n",
    "#    image_size=(img_height, img_width),\n",
    "#    seed=seed,\n",
    "#    validation_split=0.2,\n",
    "#    subset=\"training\",\n",
    "#    crop_to_aspect_ratio=False\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da5cf82",
   "metadata": {},
   "source": [
    "Now, let's look at the models results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab57a99",
   "metadata": {},
   "source": [
    "### 1) `CNN_30-30_50e`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a5d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gtsrb_exists:\n",
    "    batch_size = 32\n",
    "    img_height, img_width = 30, 30\n",
    "    \n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"data/gtsrb/Train/\",\n",
    "        label_mode='int',\n",
    "        class_names=class_names,\n",
    "        batch_size=batch_size,\n",
    "        image_size=(img_height, img_width),\n",
    "        seed=seed,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        crop_to_aspect_ratio=False\n",
    "    )\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"data/gtsrb/Train/\",\n",
    "        label_mode='int',\n",
    "        class_names=class_names,\n",
    "        batch_size=batch_size,\n",
    "        image_size=(img_height, img_width),\n",
    "        seed=seed,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        crop_to_aspect_ratio=False\n",
    "    )\n",
    "    \n",
    "    y_train, y_train_proba, y_train_pred = calculations.get_y_predictions(model, train_ds, class_names)\n",
    "    train_conf_mat = calculations.get_confusion_matrix(y_train, y_train_pred)\n",
    "    \n",
    "    y_val, y_val_proba, y_val_pred = calculations.get_y_predictions(model, val_ds, class_names)\n",
    "    val_conf_mat = calculations.get_confusion_matrix(y_val, y_val_pred)\n",
    "else:\n",
    "    train_conf_mat, val_conf_mat = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9657be",
   "metadata": {},
   "source": [
    "Here's the confusion matrix of the `CNN_30-30_50e` on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc267e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plots.plot_confusion_matrix_type(\n",
    "    \"confusion\", \"train\", class_names, \n",
    "    conf_mat=train_conf_mat, \n",
    "    output_folderpath=\"imgs/CNN_30-30_50e/\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd86b8",
   "metadata": {},
   "source": [
    "So as you can guess, a good model will have high value on the diagonal, because diagonal values are the right predicted labels.\n",
    "\n",
    "Here we can't really see at the errors because of the colorscale. So a good way to visualize them is to divided each row (actual label) by its sum (the number of predictions for it), and remove all diagonal values. So we get prediction errors proportions (or percentages by multipling by 100) belonging to each actual class.\n",
    "\n",
    "Let's see it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8693a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plots.plot_confusion_matrix_type(\n",
    "    \"errors\", \"train\", class_names, \n",
    "    conf_mat=train_conf_mat, \n",
    "    output_folderpath=\"imgs/CNN_30-30_50e/\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf79bf3",
   "metadata": {},
   "source": [
    "Here's why it was hard to see errors: there are few of them. \n",
    "The higher error is around 0.5%, which is very low (but keep in mind that we're looking at the training set results).\n",
    "\n",
    "Let's analyze the errors:\n",
    "\n",
    "- 0.50% of class n°42 images (*end no overtaking by heavy goods vehicles*) have been predicted as class n°6 (*end speed limit 80*). \n",
    "\n",
    "Let's see random images from these classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_class_confusion(60, 10, 6, \"42\", \"6\", \"data/gtsrb/Train/\", \n",
    "    output=\"\", set_type=\"train\", random_state=123, gtsrb_exists=gtsrb_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5a773",
   "metadata": {},
   "source": [
    "Of course these classes have the stripes in common that could be a reason why some confusions happened.\n",
    "\n",
    "- 0.46% of class n°29 images (*cyclists*) have been predicted as class n°26 (*traffic signals*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c9bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_class_confusion(60, 10, 6, \"29\", \"26\", \"data/gtsrb/Train/\", \n",
    "    output=\"\", set_type=\"train\", random_state=123, gtsrb_exists=gtsrb_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a29fcec",
   "metadata": {},
   "source": [
    "Here we could say that the poor quality of some images could lead to confusion.\n",
    "\n",
    "- 0.46% of class n°29 images (*cyclists*) have been predicted as class n°31 (*wild animals*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19d40ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_class_confusion(60, 10, 6, \"29\", \"31\", \"data/gtsrb/Train/\", \n",
    "    output=\"\", set_type=\"train\", random_state=123, gtsrb_exists=gtsrb_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe60d12",
   "metadata": {},
   "source": [
    "Here, we could say that the symbols are a bit similar in a way.\n",
    "\n",
    "Let's see the results on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e230c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plots.plot_confusion_matrix_type(\n",
    "    \"errors\", \"val\", class_names, \n",
    "    conf_mat=val_conf_mat, \n",
    "    output_folderpath=\"imgs/CNN_30-30_50e/\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1248cb6c",
   "metadata": {},
   "source": [
    "Errors percentages are higher, but still low.\n",
    "\n",
    "There are others types of errors here, let's look at the bigest:\n",
    "\n",
    "* 4.54% of class n°27 images (*pedestrians*) have been predicted as class n°11 (*crossroads minor road*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ffff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_class_confusion(60, 10, 6, \"27\", \"11\", \"data/gtsrb/Train/\", \n",
    "    output=\"\", set_type=\"val\", random_state=123, gtsrb_exists=gtsrb_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18263f35",
   "metadata": {},
   "source": [
    "The confusion is easily understandable: same sign shape and symbols really similar (considering the dimension reduction at 30 by 30 pixels)\n",
    "    \n",
    "* 4.17% of class n°0 images (*speed limit 20*) have been predicted as class n°1 (*speed limit 30*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd7e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_class_confusion(60, 10, 6, \"0\", \"1\", \"data/gtsrb/Train/\", \n",
    "    output=\"\", set_type=\"val\", random_state=123, gtsrb_exists=gtsrb_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d6271",
   "metadata": {},
   "source": [
    "Here the only change is the digits 2 becoming 3, which are close shapes and even more when you have low size/dimensions images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c6633",
   "metadata": {},
   "source": [
    "### 2) `MobileNet_224-224_120e_DA-50e`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0b5026",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gtsrb_exists:\n",
    "    batch_size = 32\n",
    "    img_height, img_width = 224, 224\n",
    "    \n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"data/gtsrb/Train/\",\n",
    "        label_mode='int',\n",
    "        class_names=class_names,\n",
    "        batch_size=batch_size,\n",
    "        image_size=(img_height, img_width),\n",
    "        seed=seed,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        crop_to_aspect_ratio=False\n",
    "    )\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"data/gtsrb/Train/\",\n",
    "        label_mode='int',\n",
    "        class_names=class_names,\n",
    "        batch_size=batch_size,\n",
    "        image_size=(img_height, img_width),\n",
    "        seed=seed,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        crop_to_aspect_ratio=False\n",
    "    )\n",
    "    \n",
    "    y_train, y_train_proba, y_train_pred = calculations.get_y_predictions(model, train_ds, class_names)\n",
    "    train_conf_mat = calculations.get_confusion_matrix(y_train, y_train_pred)\n",
    "    \n",
    "    y_val, y_val_proba, y_val_pred = calculations.get_y_predictions(model, val_ds, class_names)\n",
    "    val_conf_mat = calculations.get_confusion_matrix(y_val, y_val_pred)\n",
    "else:\n",
    "    train_conf_mat, val_conf_mat = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa995bdd",
   "metadata": {},
   "source": [
    "Here's the errors matrix of the `MobileNet_224-224_120e_DA-50e` model, on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aae3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plots.plot_confusion_matrix_type(\n",
    "    \"errors\", \"train\", class_names, \n",
    "    conf_mat=train_conf_mat, \n",
    "    output_folderpath=\"imgs/MobileNet_224-224_120e_DA-50e/\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef737984",
   "metadata": {},
   "source": [
    "There are few confusions, and the greatest is around 1.85% with the class n°29 (*cyclists*) predicted as class n°13 (*give way*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e60e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_class_confusion(60, 10, 6, \"29\", \"13\", \"data/gtsrb/Train/\", \n",
    "    output=\"\", set_type=\"train\", random_state=123, gtsrb_exists=gtsrb_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0737326c",
   "metadata": {},
   "source": [
    "This confusion isn't obvious because the sign shapes are inverted and the first has a symbol unlike the second which is empty.\n",
    "\n",
    "Now, the error matrix on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plots.plot_confusion_matrix_type(\n",
    "    \"errors\", \"val\", class_names, \n",
    "    conf_mat=val_conf_mat, \n",
    "    output_folderpath=\"imgs/MobileNet_224-224_120e_DA-50e/\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79e4bf8",
   "metadata": {},
   "source": [
    "The errors are a bit higher but not as much as with the CNN model.\n",
    "\n",
    "The highest one is 2.27% with class n°27 (*pedestrians*) predicted as a class n°18 (*other danger*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e12629",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_class_confusion(60, 10, 6, \"27\", \"18\", \"data/gtsrb/Train/\", \n",
    "    output=\"\", set_type=\"train\", random_state=123, gtsrb_exists=gtsrb_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c30d832",
   "metadata": {},
   "source": [
    "We can guess that the confusion could come from the reshaped pedestrian image in a low resolution.\n",
    "Maybe it could look like a bar and then an exclamation mark.\n",
    "\n",
    "The second highest error is the class n°29 (*cyclists*) predicted as the class n°13 (*give way*). We've already seen this confusion with the training set results and it was the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7ad5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plots.plot_class_confusion(60, 10, 6, \"29\", \"13\", \"data/gtsrb/Train/\", \n",
    "    output=\"\", set_type=\"val\", random_state=123, gtsrb_exists=gtsrb_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70efcaa",
   "metadata": {},
   "source": [
    "Of course, for more details you can see the specific images that have been wrongly predicted (you'll see it in the next part)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46cd807",
   "metadata": {},
   "source": [
    "How to improve the model then ?\n",
    "\n",
    "* An obvious way would be to train data with higher dimensions.\n",
    "Here, our results are really good so it isn't required to continue.\n",
    "But it could improve the model, with data augmentation for more epochs than I did, it could work.\n",
    "\n",
    "* Another way could be to change the structure of the CNN models: adding layers, changing the neurons number, the fitting parameters like the optimizer and so on.\n",
    "\n",
    "* Also, it could be to find and add new data in the training set.\n",
    "\n",
    "* You could try others classification models too.\n",
    "\n",
    "* An interesting one is to create an ensemble model. For example we could create an ensemble from the the CNN and the MobileNet we've just analyzed. This method can be really efficient and improve the accuracy of the model if the models do different type of errors. Each model of an ensemble will give a prediction and then a hard or soft vote will be processed in order to have the final prediction.\n",
    "You can look in this notebook that illustrates what you can find in a chapter of the book *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* by Aurélien Géron: https://github.com/ageron/handson-ml3/blob/main/07_ensemble_learning_and_random_forests.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae5cb1",
   "metadata": {},
   "source": [
    "## 4. Test set and model selection\n",
    "\n",
    "The GTSRB dataset contains a Test directory with 12630 images.\n",
    "\n",
    "With the `Test.csv` file, you can find their actual classes.\n",
    "\n",
    "Let's load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2301710",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gtsrb_exists:\n",
    "    test_metadata = pd.read_csv(data_dir+\"Test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b97c2c",
   "metadata": {},
   "source": [
    "In order to load these images as we did with training and validation sets we need to put the images in subfolders corresponding to their classes.\n",
    "\n",
    "I coded a function for that, you can find it in the `calculations.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360be426",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7178b9bb",
   "metadata": {},
   "source": [
    "I set the `delete` value to `False`, in that case you'll have to delete all the files that are not in a class subfolder (in the Test folder).\n",
    "With `delete=False`, the function copies files in these subfolders but doesn't delete them.\n",
    "\n",
    "If `delete=True` these files will be deleted when copied (in other words, files will be moved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21772ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gtsrb_exists:\n",
    "    _ = calculations.load_test_dataset_labels_and_create_folders(\n",
    "        test_metadata, \n",
    "        test_folderpath=\"data/gtsrb/Test/\", \n",
    "        new_data_location=\"data/gtsrb/Test/\",\n",
    "        delete=delete\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c8ca8",
   "metadata": {},
   "source": [
    "Then you can load test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5505c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gtsrb_exists:\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"data/gtsrb/Test/\",\n",
    "        label_mode='int', # labels are encoded as integers\n",
    "        class_names=class_names, # list of class names (they've to match the names of subdirectories)\n",
    "        batch_size=batch_size, # batch_size\n",
    "        image_size=(img_height, img_width), # size to resize images to after they are read from disk\n",
    "        seed=seed, # seed, to ensure reproductibility (~ random_state)\n",
    "        crop_to_aspect_ratio=False # if True, resize the images without aspect ratio distortion\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21cc24e",
   "metadata": {},
   "source": [
    "As we have to load test data each time we want to get predictions, you can write this that will enable us to load it each time with the same default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133fab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gtsrb_exists:\n",
    "    batch_size = 32\n",
    "    img_height, img_width = 224, 224\n",
    "    \n",
    "    load_test_ds = partial(\n",
    "        tf.keras.utils.image_dataset_from_directory, ## the function\n",
    "        label_mode='int',                            ## the default parameters\n",
    "        class_names=class_names,                     #\n",
    "        color_mode=color_mode,                       #\n",
    "        batch_size=batch_size,                       #\n",
    "        image_size=(img_height, img_width),          #\n",
    "        shuffle=True,                                #\n",
    "        seed=seed,                                   #\n",
    "        crop_to_aspect_ratio=False                   #\n",
    "    )\n",
    "else:\n",
    "    load_test_ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6db0ee",
   "metadata": {},
   "source": [
    "So in order to just load the test data before doing predictions, we'll just call it this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831af038",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gtsrb_exists:\n",
    "    test_ds = load_test_ds(\"data/gtsrb/Test/\")\n",
    "    \n",
    "    y_test, y_test_proba, y_test_pred = calculations.get_y_predictions(model, test_ds, class_names)\n",
    "    test_conf_mat = calculations.get_confusion_matrix(y_test, y_test_pred)\n",
    "else:\n",
    "    test_conf_mat = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e42ed6",
   "metadata": {},
   "source": [
    "Let's come back to the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83119bc0",
   "metadata": {},
   "source": [
    "Here are the results on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ad712",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(\"imgs/global_accuracy_scores_train-val-test.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0c1ae4",
   "metadata": {},
   "source": [
    "The accuracies aren't as good as on training and validation sets but still.\n",
    "\n",
    "Again, the models using data augmentation don't perform better than the \"originals\", except for the MobileNet one.\n",
    "\n",
    "Note that now, the CNN models perform better when they use higher images dimensions.\n",
    "The best CNN model is then the `CNN_90-90-50e` with 97.0% accuracy.\n",
    "\n",
    "But the `MobiletNet_224-224_DA-50e` is even better with 98.0% accuracy.\n",
    "\n",
    "Let's look at its results on this test set:\n",
    "\n",
    "First, the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plots.plot_confusion_matrix_type(\n",
    "    \"confusion\", \"test\", class_names, \n",
    "    conf_mat=test_conf_mat, \n",
    "    output_folderpath=\"imgs/MobileNet_224-224_120e_DA-50e/\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f0a5ac",
   "metadata": {},
   "source": [
    "And the error matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92f60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plots.plot_confusion_matrix_type(\n",
    "    \"errors\", \"test\", class_names, \n",
    "    conf_mat=test_conf_mat, \n",
    "    output_folderpath=\"imgs/MobileNet_224-224_120e_DA-50e/\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef2728",
   "metadata": {},
   "source": [
    "There's a big error here: 19.17% of class n°22 (*uneven surface*) images have been predicted as class n°25 (*roadworks*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_class_confusion(60, 10, 6, \"22\", \"25\", \"data/gtsrb/Test/\", \n",
    "    output=\"\", set_type=\"test\", random_state=123, gtsrb_exists=gtsrb_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0087ae83",
   "metadata": {},
   "source": [
    "These symbols aren't similar but they have the right *\"bump\"* in common.\n",
    "Also we could guess that some shades on *uneven surface* signs could confuse the model and make it think that there's a person on the left like in *roadworks*.\n",
    "\n",
    "Let's see the images causing the errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed62c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "actual, predicted = 22, 25\n",
    "plots.plot_wrong_predictions(\n",
    "    load_test_ds, \"data/gtsrb/Test/\", \n",
    "    f\"imgs/confusions/MN224-224-120da50_classes-{actual}-{predicted}.png\", \n",
    "    y, y_pred, actual, predicted, batch_size, \n",
    "    gtsrb_exists=gtsrb_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781134e",
   "metadata": {},
   "source": [
    "These images contains shades and trees background. There might be some problems with that. So maybe Data Augmentation or adding more data could help to fix these errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5330c70",
   "metadata": {},
   "source": [
    "Another error that has been done 9.33% of the time is the class n°30 (*ice snow*) predicted as class n°28 (*children*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f787bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_class_confusion(60, 10, 6, \"30\", \"28\", \"data/gtsrb/Test/\", \n",
    "    output=\"\", set_type=\"test\", random_state=123, gtsrb_exists=gtsrb_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246753ec",
   "metadata": {},
   "source": [
    "Here the only reason I can think about is that some snow ice images have a poor quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "actual, predicted = 30, 28\n",
    "plots.plot_wrong_predictions(\n",
    "    load_test_ds, \"data/gtsrb/Test/\", \n",
    "    f\"imgs/confusions/MN224-224-120da50_classes-{actual}-{predicted}.png\", \n",
    "    y, y_pred, actual, predicted, batch_size, \n",
    "    gtsrb_exists=gtsrb_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec29206",
   "metadata": {},
   "source": [
    "These errors seem to happen when sings are too bright or overexposed with a really white background. It also seems to cause damages on symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8fee69",
   "metadata": {},
   "source": [
    "And, we're going to end with the 6.11% errors done for class n°17 (*no entry*) predicted as class n°12 (*priority road*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aade713",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_class_confusion(60, 10, 6, \"17\", \"12\", \"data/gtsrb/Test/\", \n",
    "    output=\"\", set_type=\"test\", random_state=123, gtsrb_exists=gtsrb_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc221ff3",
   "metadata": {},
   "source": [
    "Here again, the signs aren't similar at all, so the only reason seems to be poor quality images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b1b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "actual, predicted = 17, 12\n",
    "plots.plot_wrong_predictions(\n",
    "    load_test_ds, \"data/gtsrb/Test/\", \n",
    "    f\"imgs/confusions/MN224-224-120da50_classes-{actual}-{predicted}.png\", \n",
    "    y, y_pred, actual, predicted, batch_size, \n",
    "    gtsrb_exists=gtsrb_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57553b3",
   "metadata": {},
   "source": [
    "These images are unsaturated, this might be the reason why there are confusion.\n",
    "\n",
    "Again Data Augmentation or adding more data could help to avoid these errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8af19a0",
   "metadata": {},
   "source": [
    "Anyway! The `MobiletNet_224-224_DA-50e` model is doing great, the one giving the best results, so we'll keep it for the full process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd7d172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
